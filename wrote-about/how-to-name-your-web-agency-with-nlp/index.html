<!DOCTYPE html><html class="no-js" lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="X-UA-Compatible" content="ie=edge"><title>How to name your agency with NLP - Robb Owen</title><meta name="description" content="An introduction to some of the basic concepts behind Natural Language Processing, and how they can be put to use."><meta name="theme-color" content="#f9cc63"><meta name="color-scheme" content="only light"><link rel="apple-touch-icon" sizes="180x180" href="../../assets/img/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../../assets/img/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../../assets/img/favicon-16x16.png"><link rel="icon" type="image/png" href="../../assets/img/favicon.ico"><link href="https://front-end.social/@robb" rel="me"><meta property="og:site_name" content="Robb Owen Digital"><meta property="og:type" content="website"><meta property="og:locale" content="en_gb"><meta property="og:url" content="https://robbowen.digital/wrote-about/how-to-name-your-web-agency-with-nlp/"><meta property="og:title" content="How to name your agency with NLP - Robb Owen "><meta property="og:description" content="An introduction to some of the basic concepts behind Natural Language Processing, and how they can be put to use."><meta property="og:image" content="https://robbowen.digital/assets/img/cards/how-to-name-your-web-agency-with-nlp.png"><meta name="twitter:image" content="https://robbowen.digital/assets/img/cards/how-to-name-your-web-agency-with-nlp.png"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:creator" content="@Robb0wen"><link href="../../feed.xml" rel="alternate" type="application/rss+xml" title="RSS Feed"><script>document.querySelector('html').classList.add('js');
    document.querySelector('html').classList.remove('no-js');</script><link href="https://fonts.googleapis.com/css?family=Bitter:400,700&amp;display=swap" rel="stylesheet"><link rel="stylesheet" href="../../assets/css/styles.css"><style id="theme">:root{--bg:#f8f9fa;--line:#306998;--fill:#f9cc63;--text:#212337;--subtext:#508bbb;--duo:#f9cc63}.stripes{background-image:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 22.93 22.93'%3E%3Cpolygon fill='%23f9cc63' points='0 8.18 14.75 22.93 22.74 22.93 0 0.19 0 8.18'/%3E%3Cpolygon fill='%23f9cc63' points='22.93 8.37 22.93 0.38 22.56 0 14.56 0 22.93 8.37'/%3E%3C/svg%3E");background-size:11px;opacity:.6}.dots{background-image:url("data:image/svg+xml,%3Csvg version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 22.9 22.9' style='enable-background:new 0 0 22.9 22.9;' xml:space='preserve'%3E%3Ccircle fill='%23f9cc63' class='st0' cx='5.7' cy='5.9' r='3'/%3E%3Ccircle fill='%23f9cc63' class='st0' cx='17.2' cy='17.2' r='3'/%3E%3C/svg%3E%0A");background-size:12px}</style></head><body><a class="skip-link u-vhide" href="index.html#main-content">Skip to content</a><div class="loading loading--in"><div class="loading__mask"></div><div class="loader"><div class="lds-ellipsis"><div></div><div></div><div></div><div></div></div></div></div><div class="pagewrap"><div class="mainnav"><div class="logo-wrap"><a href="../../index.html"><span class="u-vhide">Back to the homepage</span> <span class="logo js-logo"><div class="logo__skirrid"><div class="shade" data-drift="-5 -10"><svg version="1.1" x="0px" y="0px" viewBox="0 0 256 256"><polygon class="filled-poly" points="156.2,91.5 112.7,146.5 89,116.5 33.2,187 80.7,187 144.8,187 231.8,187"></polygon></svg></div><div class="logo"><svg version="1.1" x="0px" y="0px" viewBox="0 0 256 256"><polygon class="stroke-poly" points="84,106.5 28.2,177 139.8,177 "></polygon><polygon class="stroke-poly" points="151.2,81.5 75.7,177 226.8,177 "></polygon></svg></div></div></span></a></div><div class="menu-link"><button class="menu-link__trigger js-menu-trigger" aria-expanded="false" aria-controls="main-menu"><span class="menu-link__mask"><span class="menu-link__label menu-link__label--close js-menu-label-close" aria-hidden="true">Close</span> <span class="menu-link__label menu-link__label--open js-menu-label-open" aria-hidden="false">Menu</span></span></button></div><a class="hire-me" href="mailto:hello@robbowen.digital?subject=ðŸ¤˜ Hi Robb, I'd like to hire you"><div class="shade" data-drift="-4 -6" data-drift-center="y"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 60 60"><path class="filled-path" d="M30.78,9.87c13.52,0,24.46,9.58,24.46,21.41a19.4,19.4,0,0,1-5,12.95h0l2.9,9.82L42.37,50.1h0a27.51,27.51,0,0,1-11.59,2.58,26.84,26.84,0,0,1-14-3.86C10.42,45,6.24,38.52,6.24,31.2,6.24,19.53,17.26,9.87,30.78,9.87Z"></path></svg></div><div class="main"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 60 60"><path class="stroke-path" d="M27.78,5.87c13.52,0,24.46,9.58,24.46,21.41a19.4,19.4,0,0,1-5,12.95h0l2.9,9.82L39.37,46.1h0a27.51,27.51,0,0,1-11.59,2.58,26.84,26.84,0,0,1-14-3.86C7.42,41,3.24,34.52,3.24,27.2,3.24,15.53,14.26,5.87,27.78,5.87Z"></path></svg></div>Hire me</a></div><div class="main-menu js-menu" id="main-menu"><div class="main-menu__mask"></div><div class="main-menu__flyout"><div class="main-menu__inr wrap"><ul class="social-links"><li class="social-links__item"><a class="social-links__link social-links__link--ma" href="https://front-end.social/@Robb" rel="noopener noreferrer" target="_blank">Follow me on Mastodon</a></li><li class="social-links__item"><a class="social-links__link social-links__link--in" href="http://www.instagram.com/robb0wen" rel="noopener noreferrer" target="_blank">Follow me on Instagram</a></li><li class="social-links__item"><a class="social-links__link social-links__link--gh" href="https://github.com/robb0wen" rel="noopener noreferrer" target="_blank">Visit my GitHub</a></li><li class="social-links__item"><a class="social-links__link social-links__link--rss" href="../../feed.xml" rel="noopener noreferrer" target="_blank" data-rekishi-skip="">Subscribe to my RSS feed</a></li><li class="social-links__item"><a class="social-links__link social-links__link--email" href="mailto:hello@robbowen.digital" rel="noopener noreferrer" target="_blank">Send me an Email</a></li></ul><ul class="main-menu__links primary-links"><li class="primary-links__item"><a aria-current="page" class="primary-links__link t-primary" href="../../index.html" aria-describedby="desc_home">Home</a> <span id="desc_home" class="primary-links__summary">Back to the home page.</span></li><li class="primary-links__item"><a aria-current="page" class="primary-links__link t-primary" href="../../work" aria-describedby="desc_work">Work</a> <span id="desc_work" class="primary-links__summary">My approach to development.</span></li><li class="primary-links__item"><a class="primary-links__link t-primary" href="../../about" aria-describedby="desc_about">About</a> <span id="desc_about" class="primary-links__summary">A little about me and my background.</span></li><li class="primary-links__item"><a class="primary-links__link t-primary" href="../../writing" aria-describedby="desc_writing">Writing</a> <span id="desc_writing" class="primary-links__summary">My latest writing on tech and language.</span></li></ul></div></div></div><main id="main-content" class="page-body" tabindex="-1"><svg style="position:absolute;top:-900px;left:-900px;clip:rect(0,0,0,0)"><filter id="duotone_filter"><feColorMatrix type="matrix" result="grayscale" values="1 0 0 0 0
                  1 0 0 0 0
                  1 0 0 0 0
                  0 0 0 1 0"></feColorMatrix><feComponentTransfer color-interpolation-filters="sRGB" result="duotone"><feFuncR type="table" tableValues="0.48627450980392156 1.196078431372549"></feFuncR><feFuncG type="table" tableValues="0.2980392156862745 0.9411764705882353"></feFuncG><feFuncB type="table" tableValues="-0.1450980392156863 0.33725490196078434"></feFuncB><feFuncA type="table" tableValues="0 1"></feFuncA></feComponentTransfer></filter></svg><div class="article" data-reveal="is-onscreen"><div><div class="article-heading"><span class="article-heading__byline">18th April 2020</span><h1 class="article-heading__title t-heading t-medium:s t-big:m t-bigger:l"><span>How to name your agency with NLP<span class="dot">.</span></span></h1></div></div><div class="article__content flow reveal-content"><div class="break-out"><div class="reveal-img dots" data-src="/assets/img/articles/nlp-diagram.jpg" style="padding-top:52.62008733624454%"><img class="" src="../../assets/img/articles/nlp-diagram.jpg" alt="An abstract representation of an NLP dependency graph"></div></div><p>As with so many things these days, it starts with clickbait.</p><p>"17 tricks to running a digital agency" was, for the most part, the same kind of unremarkable listicle that surfaces on Twitter from time to time. You know the sort - it keeps things light, you chuckle at a few zingers about office-dogs, exposed piping and wall-mounted fixies. You hit retweet and carry on with the day. This time though, there was one section that stood out to me. On naming your agency, it read:</p><p><strong>Just pick a random word or two, and go with that</strong></p><p>Could it really be that simple? Let's find out.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="striking-the-right-tone">Striking the right tone<span class="dot">.</span><a href="index.html#striking-the-right-tone" class="u-vhide"> permalink</a></h2><p><em>Random</em> is a pretty strong word.</p><p>Think about the web agencies that you admire. The best names might <em>seem</em> random but usually carry some extra weight or depth behind them. This is all perhaps quite obvious, but it pokes an immediate flaw in our meme-ified advice. How can our choice be random, but still retain some deeper meaning?</p><p>Fortunately for our soon-to-be-industry-leading web agency, there's a whole branch of machine learning that can help us automate exactly this kind of problem: <strong>Natural Language Processing</strong>.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="an-introduction-to-nlp">An introduction to NLP<span class="dot">.</span><a href="index.html#an-introduction-to-nlp" class="u-vhide"> permalink</a></h2><p>Natural Language Processing, or <strong>NLP</strong>, is all about training computers to process and understand human languages. At this point you might be thinking that teaching language to humans is already pretty difficult and, honestly, you wouldn't be wrong. NLP is a <em>really</em> dense topic, but building even the smallest NLP applications can feel hugely empowering.</p><p>That said, NLP tutorials have a tendency to get very technical, very quickly and they often assume that anyone entering the field is coming armed with a dual-PhD in linguistics and computer science. On top of that, NLP and machine learning are often cushioned by a thick layer of marketing BS that makes it hard to separate reality from building SkyNet. It's clear that NLP is an exciting topic but, for newcomers, all of the above means that it isn't always clear what you might actually want to do with it.</p><p>In this tutorial, we're going to start slow and ease into a few core NLP concepts. We'll write a short script to 'borrow' the tone of voice from other agencies that we like. We'll do this by scraping their websites and plucking a list of nouns and adjectives from the text content. Finally, as the original meme suggested, we'll randomly mash some of those words together to generate a few names for our awesome new web agency.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="pre-requisites-and-setup">Pre-requisites and setup<span class="dot">.</span><a href="index.html#pre-requisites-and-setup" class="u-vhide"> permalink</a></h2><p>Python is considered to be the de facto place to start for natural language processing and to generate our agency name, we're going to need Python 3.7. We'll also need to manage our dependencies with a Pipfile (for the JavaScript fluent amongst you: think <em>package.json</em>). For that, we're also going to need <strong>pipenv</strong>.</p><div class="reveal-img dots" data-src="/assets/img/articles/nlp-python.jpg" style="padding-top:52.62008733624454%"><img class="" src="../../assets/img/articles/nlp-python.jpg" alt="The lemma for goes going went and gone in go"></div><p>If you're new to Python, then you can install everything you need with the brilliant <a href="http://docs.python-guide.org/en/latest/starting/installation/" target="_blank">Hitchhiker's guide</a>. Once you have installed Python itself, the guide has <a href="https://docs.python-guide.org/dev/virtualenvs/#virtualenvironments-ref" target="_blank">a handy page explaining how to install pipenv</a>. This tutorial will assume some knowledge of Python syntax, but I'll keep the code as simple as possible.</p><p>With everything installed, create a project directory and we're ready to get going.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="the-corpus">The Corpus<span class="dot">.</span><a href="index.html#the-corpus" class="u-vhide"> permalink</a></h2><p>Before we can process any text content, we first need to find a suitable a body of text. In NLP terminology this input text is known as our '<strong>corpus</strong>'.</p><p>In our case, we want to grab our text content from other web agencies that we admire, so we can kick things off by writing a function to grab text content from a remote url. Fortunately the Beautiful Soup package helps us to do this.</p><p>Let's install Beautiful Soup, and the HTML5 library to our project dependencies. Open the project directory in your terminal and run:</p><pre class="language-c"><code class="language-c"><span class="highlight-line">pipenv install bs4 html5lib</span></code></pre><p>Next, create a file called <code>main.py</code> with the following contents:</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">import</span> requests <span class="token comment"># we'll need this to make requests to URLs</span></span><br><span class="highlight-line"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token punctuation">,</span> Comment <span class="token comment"># HTML and comment parsers</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token keyword">def</span> <span class="token function">read_urls</span><span class="token punctuation">(</span>urls<span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">  content_string <span class="token operator">=</span> <span class="token string">''</span></span><br><span class="highlight-line">  <span class="token keyword">for</span> page <span class="token keyword">in</span> urls<span class="token punctuation">:</span></span><br><span class="highlight-line">    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Scraping URL: </span><span class="token interpolation"><span class="token punctuation">{</span>page<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span></span><br><span class="highlight-line">    <span class="token comment"># Synchronous request to page, and store returned markup</span></span><br><span class="highlight-line">    scrape <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>page<span class="token punctuation">)</span><span class="token punctuation">.</span>content</span><br><span class="highlight-line"></span><br><span class="highlight-line">    <span class="token comment"># Parse the returned markup with BeautifulSoup's html5 parser</span></span><br><span class="highlight-line">    soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>scrape<span class="token punctuation">,</span> <span class="token string">"html5lib"</span><span class="token punctuation">)</span></span><br><span class="highlight-line">    </span><br><span class="highlight-line">    <span class="token comment"># loop script tags and remove them</span></span><br><span class="highlight-line">    <span class="token keyword">for</span> script_tag <span class="token keyword">in</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'script'</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">      script_tag<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line">    <span class="token comment"># loop style tags and remove them</span></span><br><span class="highlight-line">    <span class="token keyword">for</span> style_tag <span class="token keyword">in</span> soup<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'style'</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">      style_tag<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line">    <span class="token comment"># find all comments and remove them</span></span><br><span class="highlight-line">    <span class="token keyword">for</span> comment <span class="token keyword">in</span> soup<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token keyword">lambda</span> text<span class="token punctuation">:</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> Comment<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">      comment<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line">    <span class="token comment"># Join all the scraped text content</span></span><br><span class="highlight-line">    text <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>soup<span class="token punctuation">.</span>findAll<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span><br><span class="highlight-line">    <span class="token comment"># Append to previously scraped text</span></span><br><span class="highlight-line">    content_string <span class="token operator">+=</span> text</span><br><span class="highlight-line"></span><br><span class="highlight-line">  <span class="token keyword">return</span> content_string</span></code></pre><p>You don't need to worry too much about the specific code of the function above, but it goes as follows: It read a list of URLs, making a request to each before parsing the content with BeautifulSoup. The scraped markup needs to be tidied up a bit though, so next it removes any comments, script or style tags before pulling out the remaining text content and appending it to a string. When every URL in the list has been scraped, the string is returned as our corpus.</p><p>Now we have our corpus, we can start to process it.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="lost-in-spacy">Lost in SpaCy<span class="dot">.</span><a href="index.html#lost-in-spacy" class="u-vhide"> permalink</a></h2><p><a href="https://spacy.io/" target="_blank">SpaCy</a> bills itself as 'Industrial-strength Natural Language Processing in Python'. Whereas packages like <a href="https://www.nltk.org" target="_blank">NLTK</a> offer immense amounts of flexibility and configuration, the flipside is a steep learning curve. SpaCy on the other hand, can be set up and ready to go in minutes.</p><p>We need to add SpaCy to our project as a dependency. Open your project directory in your terminal and run:</p><pre class="language-c"><code class="language-c"><span class="highlight-line">pipenv install spacy</span></code></pre><p>SpaCy doesn't do much by itself though - it needs a pre-trained language model to work with. Fortunately several are available out of the box. We'll also install the standard english model and save it to our Pipfile:</p><pre class="language-c"><code class="language-c"><span class="highlight-line">pipenv install https<span class="token operator">:</span><span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>explosion<span class="token operator">/</span>spacy<span class="token operator">-</span>models<span class="token operator">/</span>releases<span class="token operator">/</span>download<span class="token operator">/</span>en_core_web_sm<span class="token operator">-</span><span class="token number">2.2</span><span class="token number">.5</span><span class="token operator">/</span>en_core_web_sm<span class="token operator">-</span><span class="token number">2.2</span><span class="token number">.5</span><span class="token punctuation">.</span>tar<span class="token punctuation">.</span>gz </span></code></pre><p>With our dependencies installed, let's get back into the code. Open up <code>main.py</code> and add SpaCy to our imports at the top:</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">import</span> requests <span class="token comment"># we'll need this to make requests to URLs</span></span><br><span class="highlight-line"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token punctuation">,</span> Comment <span class="token comment"># HTML and comment parsers</span></span><br><span class="highlight-line"><span class="token keyword">import</span> en_core_web_sm <span class="token comment"># SpaCy's english model</span></span></code></pre><p>Next, underneath our <code>read_urls</code> function, we can define a new function to allow SpaCy to parse the corpus.</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">def</span> <span class="token function">get_tokens</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">  <span class="token comment"># load the pre-trained english model</span></span><br><span class="highlight-line">  nlp <span class="token operator">=</span> en_core_web_sm<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span></span><br><span class="highlight-line">  <span class="token comment"># parse the corpus and return the processed data</span></span><br><span class="highlight-line">  <span class="token keyword">return</span> nlp<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span></span></code></pre><p>Notice the name of our function, <code>get_tokens</code>. What's that all about?</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="parts-of-speech-and-tokenization">Parts of Speech and Tokenization<span class="dot">.</span><a href="index.html#parts-of-speech-and-tokenization" class="u-vhide"> permalink</a></h2><p>Most languages divide their words up into different categories. These usually comprise nouns, verbs, adjectives, adverbs and more. In linguistic terms, these categories are known as 'parts of speech', or POS. This is important to us as, to generate our agency name, we want to single out nouns and adjectives. But how can we get at them?</p><p>One place to start would be to break each sentence up into its individual parts but this isn't a straightforward as using <code>String split()</code>. We need a way to intelligently divide our text, whilst retaining the individual purpose of each word. Doing this manually would be too time consuming, so this is where machine learning becomes necessary. Most NLP libraries use trained language data to intelligently scan, compare and identify the characteristics of each word. Once identified, these words are separated out into 'tokens'. This process is known as <strong>tokenization</strong> and is one of the most important core principles of NLP.</p><p>Our <code>get_tokens</code> function tokenizes our corpus. Or, in simpler terms, SpaCy reads over the corpus, splits out individual words and returns a list of tokens. In creating that list, SpaCy also adds a lot of extra information to each extracted word, such as its relationship to other words in the sentence or which part of speech it is. We can use those attributes to filter the list down to find a specific part of speech.</p><p>Let's create another function:</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">def</span> <span class="token function">get_word_list</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> part_of_speech<span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">  <span class="token comment"># extract a list of words and capitalise them</span></span><br><span class="highlight-line">  list_of_words <span class="token operator">=</span> <span class="token punctuation">[</span></span><br><span class="highlight-line">    word<span class="token punctuation">.</span>text<span class="token punctuation">.</span>capitalize<span class="token punctuation">(</span><span class="token punctuation">)</span> </span><br><span class="highlight-line">    <span class="token keyword">for</span> word <span class="token keyword">in</span> tokens </span><br><span class="highlight-line">    <span class="token keyword">if</span> word<span class="token punctuation">.</span>pos_ <span class="token operator">==</span> part_of_speech</span><br><span class="highlight-line">  <span class="token punctuation">]</span></span><br><span class="highlight-line">  </span><br><span class="highlight-line">  <span class="token comment"># make the list unique by casting it to a set</span></span><br><span class="highlight-line">  unique_list <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>list_of_words<span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line">  <span class="token comment"># cast it back to a list and return</span></span><br><span class="highlight-line">  <span class="token keyword">return</span> <span class="token builtin">list</span><span class="token punctuation">(</span>unique_list<span class="token punctuation">)</span></span></code></pre><p>This function receives the list of tokens, a part of speech code, and then filters the results using a <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" target="_blank">list comprehension</a> (again, for JS developers, a list comprehension is a bit like a cross between <code>Array.map</code> and <code>Array.filter</code>). To break this list comprehension down, it's best to work from the inside out â€“ For every word in our tokens list, we grab <code>word.text</code> and capitalise it, but only if the part of speech (<code>word.pos_</code>) matches our POS code parameter. You can find a full list of these POS codes in the <a href="https://spacy.io/api/annotation#pos-tagging" target="_blank">SpaCy POS documentation</a>. If you're curious about what other attributes might be available on each token, you can <a href="https://spacy.io/api/token#attributes" target="_blank">read the Token reference</a>.</p><p>Lastly, we can filter out any duplicated words by casting the results to a <code>set()</code>. We'll need to perform some list actions later though, so it is cast back to a list before being returned.</p><p>This is all looking great and we can now use this function to create a list of nouns, adjectives, verbs or adverbs. There's a slight problem though - if you were to use this function as-is, you might find that the list of words you get back is a bit of a mess.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="stop-words">Stop words<span class="dot">.</span><a href="index.html#stop-words" class="u-vhide"> permalink</a></h2><p>Most languages have a set of core words that occur frequently. In English, this might be words like "the", "at", "from", "in", "for" and so on. Whilst these words are useful for humans, they can quite quickly get in the way extracting meaning with NLP. As a result, it's common to filter them out and these kinds of words are known as <strong>stop words</strong>.</p><p>Our script is no different and the stop words need to go. Whilst we're here, we can also get rid of any rogue punctuation. Fortunately SpaCy tokens have two handy attributes that allow us to do this - the aptly named <code>is_stop</code> and <code>is_punct</code>. Let's update the list comprehension in our <code>get_word_data</code> function:</p><pre class="language-python"><code class="language-python"><span class="highlight-line">  list_of_words <span class="token operator">=</span> <span class="token punctuation">[</span></span><br><span class="highlight-line">    word<span class="token punctuation">.</span>text<span class="token punctuation">.</span>capitalize<span class="token punctuation">(</span><span class="token punctuation">)</span> </span><br><span class="highlight-line">    <span class="token keyword">for</span> word <span class="token keyword">in</span> tokens </span><br><span class="highlight-line">    <span class="token keyword">if</span> word<span class="token punctuation">.</span>pos_ <span class="token operator">==</span> part_of_speech </span><br><span class="highlight-line">    <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>is_stop</span><br><span class="highlight-line">    <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>is_punct</span><br><span class="highlight-line">  <span class="token punctuation">]</span></span></code></pre><p>Now we can extract a nice sanitised list of words, but there's a tiny bit more optimisation that we can do.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="lemmatization">Lemmatization<span class="dot">.</span><a href="index.html#lemmatization" class="u-vhide"> permalink</a></h2><p>Let's say you've scraped your website content and in amongst your list of tokens are the words "goes", "going", "went" and "gone". That's quite a few words to express the same base idea. We can make this simpler.</p><p>These words could all be lumped together under the core concept: "go". In linguistic terms, we call this a word's '<strong>lemma</strong>' or the uninflected root form of a word.</p><p>The simplest way to think of a lemma is to see it as what you might look for in a dictionary:</p><div class="reveal-img dots" data-src="/assets/img/articles/nlp-lemma.jpg" style="padding-top:67.77277840269966%"><img class="" src="../../assets/img/articles/nlp-lemma.jpg" alt="The lemma for goes going went and gone in go"></div><p>Out of the box, SpaCy provides a convenient means to convert any matched term back to its lemma. This is called <strong>lemmatization</strong>. Let's hone our list of tokens down into a much tighter list of core ideas by updating our list comprehension. We'll change the first line to call <code>.lemma_</code> instead of <code>.text</code>.</p><p>Your final <code>get_word_list</code> function should look like this:</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">def</span> <span class="token function">get_word_list</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> part_of_speech<span class="token punctuation">)</span><span class="token punctuation">:</span></span><br><span class="highlight-line">  <span class="token comment"># extract a list of lemmas and capitalise them</span></span><br><span class="highlight-line">  list_of_words <span class="token operator">=</span> <span class="token punctuation">[</span></span><br><span class="highlight-line">    word<span class="token punctuation">.</span>lemma_<span class="token punctuation">.</span>capitalize<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># &lt;-- update .text to .lemma_</span></span><br><span class="highlight-line">    <span class="token keyword">for</span> word <span class="token keyword">in</span> tokens </span><br><span class="highlight-line">    <span class="token keyword">if</span> word<span class="token punctuation">.</span>pos_ <span class="token operator">==</span> part_of_speech </span><br><span class="highlight-line">    <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>is_stop</span><br><span class="highlight-line">    <span class="token keyword">and</span> <span class="token keyword">not</span> word<span class="token punctuation">.</span>is_punct</span><br><span class="highlight-line">  <span class="token punctuation">]</span></span><br><span class="highlight-line">  </span><br><span class="highlight-line">  <span class="token comment"># make the list unique by casting it to a set</span></span><br><span class="highlight-line">  unique_list <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>list_of_words<span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line">  <span class="token comment"># cast it back to a list and return</span></span><br><span class="highlight-line">  <span class="token keyword">return</span> <span class="token builtin">list</span><span class="token punctuation">(</span>unique_list<span class="token punctuation">)</span></span></code></pre><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="bringing-it-all-together">Bringing it all together<span class="dot">.</span><a href="index.html#bringing-it-all-together" class="u-vhide"> permalink</a></h2><p>Now we've set up a few functions and we've covered the core concepts, let's pull everything together. To generate the final name, we're going to need to be able to make random selections from our word lists. At the top of <code>main.py</code>, add <code>random</code> to your imports.</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token keyword">import</span> requests <span class="token comment"># we'll need this to make requests to URLs</span></span><br><span class="highlight-line"><span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token punctuation">,</span> Comment <span class="token comment"># HTML and comment parsers</span></span><br><span class="highlight-line"><span class="token keyword">import</span> en_core_web_sm <span class="token comment"># SpaCy's english model</span></span><br><span class="highlight-line"><span class="token keyword">import</span> random</span></code></pre><p>At the end of the file, underneath your <code>get_word_list</code> function, paste the following:</p><pre class="language-python"><code class="language-python"><span class="highlight-line"><span class="token comment"># create a list of urls to scrape</span></span><br><span class="highlight-line"><span class="token comment"># For illustration, I'm using my website </span></span><br><span class="highlight-line"><span class="token comment"># but replace with any urls of your choice</span></span><br><span class="highlight-line">CORPUS <span class="token operator">=</span> read_urls<span class="token punctuation">(</span><span class="token punctuation">[</span></span><br><span class="highlight-line">  <span class="token string">'https://robbowen.digital/'</span><span class="token punctuation">,</span></span><br><span class="highlight-line">  <span class="token string">'https://robbowen.digital/work'</span><span class="token punctuation">,</span></span><br><span class="highlight-line">  <span class="token string">'https://robbowen.digital/about'</span></span><br><span class="highlight-line"><span class="token punctuation">]</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token comment"># parse the corpus and tokenize</span></span><br><span class="highlight-line">TOKENS <span class="token operator">=</span> get_tokens<span class="token punctuation">(</span>CORPUS<span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token comment"># extract a unique list of noun lemmas from the token list</span></span><br><span class="highlight-line">nouns <span class="token operator">=</span> get_word_list<span class="token punctuation">(</span>TOKENS<span class="token punctuation">,</span> <span class="token string">"NOUN"</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token comment"># extract a unique list of adjective lemmas from the token list</span></span><br><span class="highlight-line">adjectives <span class="token operator">=</span> get_word_list<span class="token punctuation">(</span>TOKENS<span class="token punctuation">,</span> <span class="token string">"ADJ"</span><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token comment"># create a random list of 500 nouns and adjective-noun combos</span></span><br><span class="highlight-line"><span class="token comment"># cast to a set to make sure that we only get unique results</span></span><br><span class="highlight-line">list_of_combos <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span></span><br><span class="highlight-line">  <span class="token punctuation">[</span></span><br><span class="highlight-line">    <span class="token comment"># String interpolation of an adjective and a noun...</span></span><br><span class="highlight-line">    <span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>adjectives<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>nouns<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span> </span><br><span class="highlight-line">    <span class="token comment"># 50% of the time...</span></span><br><span class="highlight-line">    <span class="token keyword">if</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">True</span></span><br><span class="highlight-line">    <span class="token comment"># else return a random noun</span></span><br><span class="highlight-line">    <span class="token keyword">else</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>nouns<span class="token punctuation">)</span></span><br><span class="highlight-line">    <span class="token comment"># and repeat this 500 times</span></span><br><span class="highlight-line">    <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span></span><br><span class="highlight-line">  <span class="token punctuation">]</span></span><br><span class="highlight-line"><span class="token punctuation">)</span></span><br><span class="highlight-line"></span><br><span class="highlight-line"><span class="token comment"># Write list of combinations to a text file</span></span><br><span class="highlight-line">output_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./output.txt"</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span></span><br><span class="highlight-line"><span class="token keyword">for</span> item <span class="token keyword">in</span> list_of_combos<span class="token punctuation">:</span></span><br><span class="highlight-line">  output_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{</span>item<span class="token punctuation">}</span></span><span class="token string">\r\n'</span></span><span class="token punctuation">)</span></span></code></pre><p>Let's break that code down.</p><p>First of all, we pass a list of URLs into our <code>read_urls</code> function and store the result in a constant for later. Be sure to change the example URLs to your own choices here, and remember: the more text on your chosen pages, the better your results will be.</p><p>Next, we pass the corpus into our <code>get_tokens</code> function. This is where SpaCy does its magic, tokenizing the corpus and returning a list of data-rich tokens.</p><p>To generate our agency name, we want some random nouns. Nouns by themselves aren't much fun, so we'll grab some adjectives too. This uses the <code>get_word_list</code> function we created earlier. The first argument is the list of tokens, and the second filters by a '<a href="https://spacy.io/api/annotation#pos-tagging" target="_blank">part of speech code</a>'.</p><p>Now we have our nouns and adjectives lists we can prep our final list of names. We want to generate a list of names â€“ let's say 500 of them. We can use another list comprehension to do this.</p><p>Remember the random library we imported above? It gives us access to the <code>random.choice</code> function. As the name implies, it allows us to randomly choose a single item from a list. <code>random.choice</code> also gives us a handy way to embed a 50/50 chance into our comprehension. Here, an if-else means each iteration of our loop will return either an adjective-noun combo or a single noun. Finally, with the comprehension complete, we wrap the list in <code>set()</code> to ensure that we only have unique results.</p><p>Ok, everything is ready. At the very end we create a file called <code>output.txt</code>, loop through our list of names and write each to a new line.</p><p>And...we're done! If you need to, you can double check your code against the <a href="https://github.com/robb0wen/disruptive-breakfast/blob/master/main.py" target="_blank">final source on github</a>.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="generate-the-name">Generate the name<span class="dot">.</span><a href="index.html#generate-the-name" class="u-vhide"> permalink</a></h2><p>It's time to fire up the final script and generate some names. At your terminal, run the following command:</p><pre class="language-c"><code class="language-c"><span class="highlight-line">pipenv run python main<span class="token punctuation">.</span>py</span></code></pre><p>If all goes well, you should have now a file named <code>output.txt</code> chock full of multi-award-winning agency names.</p><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="and-the-winner-is">And the winner is...<span class="dot">.</span><a href="index.html#and-the-winner-is" class="u-vhide"> permalink</a></h2><p><strong>"Just pick a random word or two and go with that"</strong>. So, after all this, what name did I go with?</p><p>Well, I had some real gems: <strong>Loaf</strong>, <strong>Strategic Bear</strong>, <strong>Rise</strong>, <strong>Chaotic Rest</strong>, <strong>Bold Tomorrow</strong>. After a few minutes crawling through the output list though, I spotted <em>the one</em>.</p><p>Exciting, yet confident. Like a life-raft in a sea of comedic nonsense:</p><div class="reveal-img dots" data-src="/assets/img/articles/nlp-disruptivebreakfast.jpg" style="padding-top:67.77277840269966%"><img class="" src="../../assets/img/articles/nlp-disruptivebreakfast.jpg" alt="Disruptive Breakfast"></div><h2 class="t-heading t-lede:s t-medium:m t-medium:l" id="wrapping-up">Wrapping up<span class="dot">.</span><a href="index.html#wrapping-up" class="u-vhide"> permalink</a></h2><p>Ok, I get it. Like most clickbait, this might not have been a completely satisfying ending. Hopefully though, you've learned a little about the potential application of even the most superficial Natural Language Processing scripts.</p><p>To recap, in this introduction, we've used and learned about:</p><ul><li><strong>Corpus</strong>: The body of text to be processed</li><li><strong>Parts of speech</strong>: The name given to the categories of words comprising nouns, verbs, adjectives etc.</li><li><strong>Tokenization</strong>: The act of splitting sentences into individual words, or tokens</li><li><strong>Stop words</strong>: Small, frequently occurring words that we do not wish to include in our results i.e. "the", "for", "it" etc.</li><li><strong>Lemmatization</strong>: Changing a word back to its root concept, or dictionary form i.e. the lemma of "best" or "better" is "good"</li></ul><p>From these core concepts, the possibilities are vast. Imagine going into a pitch having scanned the client's website in advance to profile exactly how they talk about themselves. Or perhaps you could take the jump into sentiment analysis to scan how well received a particular topic is on Twitter. There's still a lot to learn but, with these core building blocks, you're on your way.</p><p>If you enjoyed this article and have any ideas on how to extend this example, or if you generated a particularly badass name, then please <a href="https://front-end.social/@Robb" target="_blank">let me know on Mastodon</a>.</p><p>If you're looking to learn more about NLP, I strongly recommend the <a href="https://course.spacy.io/" target="_blank">Advanced NLP with SpaCy</a> course by Ines Montani - it's free, and really rather good.</p></div><footer class="article__footer"><div class="author"><div class="author__inr"><div class="author__portrait"><div class="portrait"><span class="portrait__bg dots" data-drift="-10 -10"></span><div class="portrait__image"><span class="duotone"><img src="../../assets/img/portrait_small.jpg"></span></div></div></div><div class="author__bio flow"><h4 class="t-heading t-sublede:s t-lede:m t-medium:l">Hi, my name is Robb<span class="dot">.</span></h4><p>I'm a freelance creative developer helping awesome people to build ambitious yet accessible web projects.</p><a href="mailto:hello@robbowen.digital?subject=ðŸ¤˜ Hi Robb, I'd like to hire you" class="btn author__cta"><span class="btn__label">Hire me</span> <span class="btn__fill stripes" data-drift="-10 -12" aria-hidden="true">Hire me</span></a></div></div></div></footer></div></main><small class="made-in-wales">Â© MMXXIV. Gwneud yn Ne Cymru.</small></div><script src="../../assets/js/main.js"></script></body></html>